# train_and_eval.py

import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

from data_loader import Flickr8kDataset
from loss import contrastive_loss
from models.image_encoder import ImageEncoder
from models.text_encoder import TextEncoder
from utils import SimpleTokenizer


def evaluate(img_encoder, txt_encoder, dataloader, device):
    """
    è®¡ç®—ç»™å®šæ•°æ®é›†ä¸Šçš„æŸå¤±ã€‚

    Args:
        img_encoder (nn.Module): å›¾åƒç¼–ç å™¨æ¨¡å‹ã€‚
        txt_encoder (nn.Module): æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ã€‚
        dataloader (DataLoader): æ•°æ®åŠ è½½å™¨ã€‚
        device (torch.device): è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰ã€‚

    Returns:
        float: å¹³å‡æŸå¤±ã€‚
    """
    img_encoder.eval()
    txt_encoder.eval()

    total_loss = 0.0
    with torch.no_grad():
        for images, captions_ids in tqdm(dataloader, desc="Evaluating"):
            images = images.to(device)
            captions_ids = captions_ids.to(device)

            image_embeds = img_encoder(images)  # [batch, embed_dim]
            text_embeds = txt_encoder(captions_ids)  # [batch, embed_dim]

            loss = contrastive_loss(image_embeds, text_embeds)
            total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    return avg_loss


def evaluate_top_k(img_encoder, txt_encoder, dataloader, device, topk=(1, 5, 10)):
    """
    è¯„ä¼°æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„ Top-K å‡†ç¡®ç‡ã€‚

    Args:
        img_encoder (nn.Module): å›¾åƒç¼–ç å™¨æ¨¡å‹ã€‚
        txt_encoder (nn.Module): æ–‡æœ¬ç¼–ç å™¨æ¨¡å‹ã€‚
        dataloader (DataLoader): æ•°æ®åŠ è½½å™¨ã€‚
        device (torch.device): è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰ã€‚
        topk (tuple, optional): è¦è¯„ä¼°çš„ Top-K å€¼ã€‚é»˜è®¤ä¸º (1, 5, 10)ã€‚

    Returns:
        tuple: æ–‡æœ¬åˆ°å›¾åƒå’Œå›¾åƒåˆ°æ–‡æœ¬çš„ Top-K å‡†ç¡®ç‡ã€‚
    """
    img_encoder.eval()
    txt_encoder.eval()

    all_image_embeds = []
    all_text_embeds = []

    with torch.no_grad():
        for images, captions_ids in tqdm(dataloader, desc="Extracting embeddings"):
            images = images.to(device)
            captions_ids = captions_ids.to(device)

            image_embed = img_encoder(images)  # [batch, dim]
            text_embed = txt_encoder(captions_ids)  # [batch, dim]

            all_image_embeds.append(image_embed.cpu())
            all_text_embeds.append(text_embed.cpu())

    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # [N, D]
    all_text_embeds = torch.cat(all_text_embeds, dim=0)  # [N, D]

    # å½’ä¸€åŒ–
    all_image_embeds = F.normalize(all_image_embeds, dim=1)
    all_text_embeds = F.normalize(all_text_embeds, dim=1)

    # æ–‡æœ¬ -> å›¾åƒæ£€ç´¢
    sim_matrix = torch.matmul(all_text_embeds, all_image_embeds.T)  # [N, N]
    txt2img_ranks = torch.argsort(sim_matrix, dim=1, descending=True)

    # å›¾åƒ -> æ–‡æœ¬æ£€ç´¢
    sim_matrix_T = sim_matrix.T  # [N, N]
    img2txt_ranks = torch.argsort(sim_matrix_T, dim=1, descending=True)

    def recall_at_k(ranks, topk):
        recalls = []
        for k in topk:
            match = [i in ranks[i][:k] for i in range(len(ranks))]
            recalls.append(np.mean(match))
        return recalls

    r_txt2img = recall_at_k(txt2img_ranks, topk)
    r_img2txt = recall_at_k(img2txt_ranks, topk)

    print("\nğŸ“ˆ Text â†’ Image Retrieval:")
    for i, k in enumerate(topk):
        print(f"Recall@{k}: {r_txt2img[i] * 100:.2f}%")

    print("\nğŸ“ˆ Image â†’ Text Retrieval:")
    for i, k in enumerate(topk):
        print(f"Recall@{k}: {r_img2txt[i] * 100:.2f}%")

    return r_txt2img, r_img2txt


def main():
    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ–‡ä»¶è·¯å¾„ï¼Œæ ¹æ®å®é™…è°ƒæ•´
    token_file = "Flickr8k/captions.txt"  # æ€»çš„ captions æ–‡ä»¶ï¼Œç”¨äºæ„å»ºè¯è¡¨
    train_token_file = "Flickr8k/train_captions.txt"  # è®­ç»ƒé›†ï¼Œæ ¼å¼ï¼š image,caption
    val_token_file = "Flickr8k/val_captions.txt"  # éªŒè¯é›†
    test_token_file = "Flickr8k/test_captions.txt"  # æµ‹è¯•é›†

    # è¯»å–æ‰€æœ‰ caption ç”¨äºæ„å»ºæ€»è¯è¡¨ï¼ˆå‡è®¾ä»¥é€—å·åˆ†éš”ï¼Œå¦‚æœä¸æ˜¯ï¼Œè¯·ä¿®æ”¹ split å‚æ•°ï¼‰
    with open(token_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    captions = [line.strip().split(",")[1] for line in lines if line.strip()]

    # æ„å»ºç»Ÿä¸€çš„ tokenizer
    tokenizer = SimpleTokenizer(captions, min_freq=1)
    vocab_size = len(tokenizer)
    print(f"Vocabulary size: {vocab_size}")

    # æ„å»ºæ•°æ®é›†ä¸ DataLoaderï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    train_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",  # å›¾ç‰‡æ‰€åœ¨ç›®å½•
        captions_file=train_token_file,  # è®­ç»ƒé›† captions æ–‡ä»¶ï¼Œæ ¼å¼ï¼š image,caption
        tokenizer=tokenizer,
    )
    val_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",
        captions_file=val_token_file,  # éªŒè¯é›† captions æ–‡ä»¶
        tokenizer=tokenizer,
    )
    test_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",
        captions_file=test_token_file,  # æµ‹è¯•é›† captions æ–‡ä»¶
        tokenizer=tokenizer,
    )

    train_dataloader = DataLoader(
        train_dataset, batch_size=32, shuffle=True, num_workers=4, drop_last=True
    )
    # ä¸ºä¿è¯è¯„ä¼°ç¨³å®šï¼Œæ¯ä¸ª batch ä½¿ç”¨ batch_size=1
    val_dataloader = DataLoader(
        val_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False
    )

    # æ„é€ æ¨¡å‹ï¼ˆè®¾å®š embed_dim=256ï¼‰
    embed_dim = 256
    img_encoder = ImageEncoder(embed_dim=embed_dim).to(device)
    txt_encoder = TextEncoder(vocab_size, embed_dim=embed_dim).to(device)

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        list(img_encoder.parameters()) + list(txt_encoder.parameters()), lr=1e-4
    )

    best_val_loss = float("inf")
    best_val_recall = 0.0  # ç”¨äºè®°å½•æœ€ä½³éªŒè¯é›† Recall@1
    epochs = 40
    for epoch in range(epochs):
        img_encoder.train()
        txt_encoder.train()
        epoch_loss = 0.0

        for images, captions_ids in train_dataloader:
            images = images.to(device)
            captions_ids = captions_ids.to(device)

            image_embeds = img_encoder(images)  # [batch, embed_dim]
            text_embeds = txt_encoder(captions_ids)  # [batch, embed_dim]
            loss = contrastive_loss(image_embeds, text_embeds)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_train_loss = epoch_loss / len(train_dataloader)

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        val_loss = evaluate(img_encoder, txt_encoder, val_dataloader, device)
        val_recall, _ = evaluate_top_k(img_encoder, txt_encoder, val_dataloader, device)
        val_recall_at_1 = val_recall[0]  # è·å– Recall@1

        print(
            f"Epoch [{epoch + 1}/{epochs}]: Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Recall@1: {val_recall_at_1 * 100:.2f}%"
        )

        # å¦‚æœéªŒè¯é›† Recall@1 æå‡ï¼Œåˆ™ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_recall_at_1 > best_val_recall:
            best_val_recall = val_recall_at_1
            checkpoint = {
                "epoch": epoch + 1,
                "img_encoder_state_dict": img_encoder.state_dict(),
                "txt_encoder_state_dict": txt_encoder.state_dict(),
                "tokenizer_vocab": tokenizer.word2idx,
                "best_val_recall": best_val_recall,
            }
            torch.save(checkpoint, "best_clip_model.pth")
            print(
                f"    > Best model updated at epoch {epoch + 1} with Recall@1: {best_val_recall * 100:.2f}%"
            )

    # è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    final_test_loss = evaluate(img_encoder, txt_encoder, test_dataloader, device)
    print(f"Final Test Loss: {final_test_loss:.4f}")

    # æœ€ç»ˆè¯„ä¼° top-k å‡†ç¡®ç‡
    r_txt2img, r_img2txt = evaluate_top_k(
        img_encoder, txt_encoder, test_dataloader, device
    )
    print("\nğŸ“ˆ Final Test Recall@k:")
    for i, k in enumerate([1, 5, 10]):
        print(f"Text â†’ Image Recall@{k}: {r_txt2img[i] * 100:.2f}%")
        print(f"Image â†’ Text Recall@{k}: {r_img2txt[i] * 100:.2f}%")


if __name__ == "__main__":
    main()
