# train_and_eval.py
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

from data_loader import Flickr8kDataset
from loss import contrastive_loss
from models.image_encoder import ImageEncoder
from models.text_encoder import TextEncoder
from utils import SimpleTokenizer


# ä¾›åŒå­¦ä»¬å‚è€ƒ
def evaluate_top_k(img_encoder, txt_encoder, dataloader, device, topk=(1, 5, 10)):
    img_encoder.eval()
    txt_encoder.eval()

    all_image_embeds = []
    all_text_embeds = []

    with torch.no_grad():
        for images, captions_ids in tqdm(dataloader, desc="Extracting embeddings"):
            images = images.to(device)
            captions_ids = captions_ids.to(device)

            image_embed = img_encoder(images)  # [1, dim]
            text_embed = txt_encoder(captions_ids)  # [1, dim]

            all_image_embeds.append(image_embed.cpu())
            all_text_embeds.append(text_embed.cpu())

    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # [N, D]
    all_text_embeds = torch.cat(all_text_embeds, dim=0)  # [N, D]

    # å½’ä¸€åŒ–
    all_image_embeds = F.normalize(all_image_embeds, dim=1)
    all_text_embeds = F.normalize(all_text_embeds, dim=1)

    # æ–‡æœ¬ -> å›¾åƒæ£€ç´¢
    sim_matrix = torch.matmul(all_text_embeds, all_image_embeds.T)  # [N, N]
    txt2img_ranks = torch.argsort(sim_matrix, dim=1, descending=True)

    # å›¾åƒ -> æ–‡æœ¬æ£€ç´¢
    sim_matrix_T = sim_matrix.T  # [N, N]
    img2txt_ranks = torch.argsort(sim_matrix_T, dim=1, descending=True)

    def recall_at_k(ranks, topk):
        recalls = []
        for k in topk:
            match = [i in ranks[i][:k] for i in range(len(ranks))]
            recalls.append(np.mean(match))
        return recalls

    r_txt2img = recall_at_k(txt2img_ranks, topk)
    r_img2txt = recall_at_k(img2txt_ranks, topk)

    print("\nğŸ“ˆ Text â†’ Image Retrieval:")
    for i, k in enumerate(topk):
        print(f"Recall@{k}: {r_txt2img[i] * 100:.2f}%")

    return r_txt2img, r_img2txt


def main():
    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ–‡ä»¶è·¯å¾„ï¼Œæ ¹æ®å®é™…è°ƒæ•´
    token_file = "Flickr8k/captions.txt"  # æ€»çš„ captions æ–‡ä»¶ï¼Œç”¨äºæ„å»ºè¯è¡¨
    train_token_file = "Flickr8k/train_captions.txt"  # è®­ç»ƒé›†ï¼Œæ ¼å¼ï¼š image,caption
    val_token_file = "Flickr8k/val_captions.txt"  # éªŒè¯é›†
    test_token_file = "Flickr8k/test_captions.txt"  # æµ‹è¯•é›†

    # è¯»å–æ‰€æœ‰ caption ç”¨äºæ„å»ºæ€»è¯è¡¨ï¼ˆå‡è®¾ä»¥ tab åˆ†éš”ï¼Œå¦‚æœä¸æ˜¯ï¼Œè¯·ä¿®æ”¹ split å‚æ•°ï¼‰
    with open(token_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    captions = [line.strip().split(",")[1] for line in lines if line.strip()]

    # æ„å»ºç»Ÿä¸€çš„ tokenizer
    tokenizer = SimpleTokenizer(captions, min_freq=1)
    vocab_size = len(tokenizer)
    print(f"Vocabulary size: {vocab_size}")

    # æ„å»ºæ•°æ®é›†ä¸ DataLoaderï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    train_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",  # å›¾ç‰‡æ‰€åœ¨ç›®å½•
        captions_file=train_token_file,  # è®­ç»ƒé›† captions æ–‡ä»¶ï¼Œæ ¼å¼ï¼š image<TAB>caption
        tokenizer=tokenizer,
    )
    val_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",
        captions_file=val_token_file,  # éªŒè¯é›† captions æ–‡ä»¶
        tokenizer=tokenizer,
    )
    test_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",
        captions_file=test_token_file,  # æµ‹è¯•é›† captions æ–‡ä»¶
        tokenizer=tokenizer,
    )

    train_dataloader = DataLoader(
        train_dataset, batch_size=32, shuffle=True, num_workers=4, drop_last=True
    )
    # ä¸ºä¿è¯è¯„ä¼°ç¨³å®šï¼Œæ¯ä¸ª batch ä½¿ç”¨ batch_size=1
    val_dataloader = DataLoader(
        val_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=1, shuffle=False, num_workers=4, drop_last=False
    )

    # æ„é€ æ¨¡å‹ï¼ˆè®¾å®š embed_dim=256ï¼‰
    embed_dim = 256
    img_encoder = ImageEncoder(embed_dim=embed_dim).to(device)
    txt_encoder = TextEncoder(vocab_size, embed_dim=embed_dim).to(device)

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        list(img_encoder.parameters()) + list(txt_encoder.parameters()), lr=1e-4
    )

    best_val_loss = float("inf")
    epochs = 40
    for epoch in range(epochs):
        img_encoder.train()
        txt_encoder.train()
        epoch_loss = 0.0

        for images, captions_ids in train_dataloader:
            images = images.to(device)
            captions_ids = captions_ids.to(device)

            image_embeds = img_encoder(images)  # [batch, embed_dim]
            text_embeds = txt_encoder(captions_ids)  # [batch, embed_dim]
            loss = contrastive_loss(image_embeds, text_embeds)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_train_loss = epoch_loss / len(train_dataloader)

        print(
            f"Epoch [{epoch + 1}/{epochs}]: Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Test Loss: {test_loss:.4f}"
        )

        # å¦‚æœéªŒè¯é›†æœ‰æ”¹å–„ï¼Œåˆ™ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œè¿™é‡Œéœ€è¦åŒå­¦ä»¬è‡ªå·±é€‰æ‹©è¯„ä¼°æ ‡å‡†

        checkpoint = {
            "epoch": epoch + 1,
            "img_encoder_state_dict": img_encoder.state_dict(),
            "txt_encoder_state_dict": txt_encoder.state_dict(),
            "tokenizer_vocab": tokenizer.word2idx,
            "best_val_loss": best_val_loss,
        }
        torch.save(checkpoint, "best_clip_model.pth")
        print(f"    > Best model updated at epoch {epoch + 1} ")

    # è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    final_test_loss = evaluate(img_encoder, txt_encoder, test_dataloader, device)
    print(f"Final Test Loss: {final_test_loss:.4f}")


if __name__ == "__main__":
    main()
