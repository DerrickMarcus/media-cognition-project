# train_and_eval.py
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

from data_loader import Flickr8kDataset
from loss import contrastive_loss
from models.image_encoder import ImageEncoder
from models.text_encoder import TextEncoder
from utils import SimpleTokenizer


# ä¾›åŒå­¦ä»¬å‚è€ƒ
def evaluate_top_k(img_encoder, txt_encoder, dataloader, device, topk=(1, 5, 10)):
    img_encoder.eval()
    txt_encoder.eval()

    all_image_embeds = []
    all_text_embeds = []

    with torch.no_grad():
        for images, captions_ids in tqdm(dataloader, desc="Extracting embeddings"):
            images = images.to(device)
            captions_ids = captions_ids.to(device)

            image_embed = img_encoder(images)  # [1, dim]
            text_embed = txt_encoder(captions_ids)  # [1, dim]

            all_image_embeds.append(image_embed.cpu())
            all_text_embeds.append(text_embed.cpu())

    all_image_embeds = torch.cat(all_image_embeds, dim=0)  # [N, D]
    all_text_embeds = torch.cat(all_text_embeds, dim=0)  # [N, D]

    # å½’ä¸€åŒ–
    all_image_embeds = F.normalize(all_image_embeds, dim=1)
    all_text_embeds = F.normalize(all_text_embeds, dim=1)

    # æ–‡æœ¬ -> å›¾åƒæ£€ç´¢
    sim_matrix = torch.matmul(all_text_embeds, all_image_embeds.T)  # [N, N]
    txt2img_ranks = torch.argsort(sim_matrix, dim=1, descending=True)

    # å›¾åƒ -> æ–‡æœ¬æ£€ç´¢
    sim_matrix_T = sim_matrix.T  # [N, N]
    img2txt_ranks = torch.argsort(sim_matrix_T, dim=1, descending=True)

    def recall_at_k(ranks, topk):
        recalls = []
        for k in topk:
            match = [i in ranks[i][:k] for i in range(len(ranks))]
            recalls.append(np.mean(match))
        return recalls

    r_txt2img = recall_at_k(txt2img_ranks, topk)
    r_img2txt = recall_at_k(img2txt_ranks, topk)

    print("\nğŸ“ˆ Text â†’ Image Retrieval:")
    for i, k in enumerate(topk):
        print(f"Recall@{k}: {r_txt2img[i] * 100:.2f}%")

    print("\nğŸ“ˆ Image â†’ Text Retrieval:")
    for i, k in enumerate(topk):
        print(f"Recall@{k}: {r_img2txt[i] * 100:.2f}%")

    return r_txt2img, r_img2txt


def main():
    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # æ–‡ä»¶è·¯å¾„ï¼Œæ ¹æ®å®é™…è°ƒæ•´
    token_file = "Flickr8k/captions.txt"  # æ€»çš„ captions æ–‡ä»¶ï¼Œç”¨äºæ„å»ºè¯è¡¨
    train_token_file = "Flickr8k/train_captions.txt"  # è®­ç»ƒé›†ï¼Œæ ¼å¼ï¼š image,caption
    val_token_file = "Flickr8k/val_captions.txt"  # éªŒè¯é›†
    test_token_file = "Flickr8k/test_captions.txt"  # æµ‹è¯•é›†

    # è¯»å–æ‰€æœ‰ caption ç”¨äºæ„å»ºæ€»è¯è¡¨ï¼ˆå‡è®¾ä»¥ tab åˆ†éš”ï¼Œå¦‚æœä¸æ˜¯ï¼Œè¯·ä¿®æ”¹ split å‚æ•°ï¼‰
    with open(token_file, "r", encoding="utf-8") as f:
        lines = f.readlines()
    captions = [line.strip().split(",")[1] for line in lines if line.strip()]

    # æ„å»ºç»Ÿä¸€çš„ tokenizer
    tokenizer = SimpleTokenizer(captions, min_freq=1)
    vocab_size = len(tokenizer)
    print(f"Vocabulary size: {vocab_size}")

    # æ„å»ºæ•°æ®é›†ä¸ DataLoaderï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    train_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",  # å›¾ç‰‡æ‰€åœ¨ç›®å½•
        captions_file=train_token_file,  # è®­ç»ƒé›† captions æ–‡ä»¶ï¼Œæ ¼å¼ï¼š image<TAB>caption
        tokenizer=tokenizer,
    )
    val_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",
        captions_file=val_token_file,  # éªŒè¯é›† captions æ–‡ä»¶
        tokenizer=tokenizer,
    )
    test_dataset = Flickr8kDataset(
        root_dir="Flickr8k/images",
        captions_file=test_token_file,  # æµ‹è¯•é›† captions æ–‡ä»¶
        tokenizer=tokenizer,
    )

    train_dataloader = DataLoader(
        train_dataset, batch_size=512, shuffle=True, num_workers=12, drop_last=True
    )
    # ä¸ºä¿è¯è¯„ä¼°ç¨³å®šï¼Œæ¯ä¸ª batch ä½¿ç”¨ batch_size=1
    val_dataloader = DataLoader(
        val_dataset, batch_size=512, shuffle=False, num_workers=12, drop_last=False
    )
    test_dataloader = DataLoader(
        test_dataset, batch_size=512, shuffle=False, num_workers=12, drop_last=False
    )

    # æ„é€ æ¨¡å‹ï¼ˆè®¾å®š embed_dim=256ï¼‰
    embed_dim = 512
    img_encoder = ImageEncoder(embed_dim=embed_dim).to(device)
    txt_encoder = TextEncoder(
        vocab_size, embed_dim=embed_dim, encoder_type="transformer"
    ).to(device)

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        list(img_encoder.parameters()) + list(txt_encoder.parameters()), lr=1e-4
    )

    # best_val_loss = float("inf")
    best_recall = 0.0
    epochs = 30
    train_losses = []
    valid_losses = []
    for epoch in range(epochs):
        # training
        img_encoder.train()
        txt_encoder.train()
        epoch_loss = 0.0
        pbar = tqdm(
            train_dataloader, desc=f"Train Epoch {epoch + 1}/{epochs}", unit="batch"
        )

        for images, captions_ids in pbar:
            images = images.to(device)
            captions_ids = captions_ids.to(device)

            image_embeds = img_encoder(images)  # [batch, embed_dim]
            text_embeds = txt_encoder(captions_ids)  # [batch, embed_dim]
            loss = contrastive_loss(image_embeds, text_embeds)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            pbar.set_postfix(loss=loss.item())

        avg_train_loss = epoch_loss / len(train_dataloader)
        train_losses.append(avg_train_loss)

        # validation
        img_encoder.eval()
        txt_encoder.eval()
        total_val_loss = 0.0
        pbar = tqdm(
            val_dataloader, desc=f"Validation Epoch {epoch + 1}/{epochs}", unit="batch"
        )
        with torch.no_grad():
            for images, captions_ids in pbar:
                images = images.to(device)
                captions_ids = captions_ids.to(device)

                image_embeds = img_encoder(images)
                text_embeds = txt_encoder(captions_ids)
                val_loss = contrastive_loss(image_embeds, text_embeds)
                total_val_loss += val_loss.item()
                pbar.set_postfix(loss=val_loss.item())

        avg_val_loss = total_val_loss / len(val_dataloader)
        valid_losses.append(avg_val_loss)

        print(
            f"Epoch {epoch + 1}/{epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}."
        )

        topk = (1, 5, 10)
        r_txt2img, r_img2txt = evaluate_top_k(
            img_encoder, txt_encoder, val_dataloader, device, topk
        )
        avg_recall1 = (r_txt2img[0] + r_img2txt[0]) / 2
        if avg_recall1 > best_recall:
            best_recall = avg_recall1
            torch.save(
                {
                    "epoch": epoch + 1,
                    "img_encoder_state_dict": img_encoder.state_dict(),
                    "txt_encoder_state_dict": txt_encoder.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "tokenizer_vocab": tokenizer.word2idx,
                    "best_recall": best_recall,
                },
                "exp/chpts/best_clip_model_transformer.pth",
            )
            print(f"    > Best model updated at epoch {epoch + 1}.")

        # # å¦‚æœéªŒè¯é›†æœ‰æ”¹å–„ï¼Œåˆ™ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œè¿™é‡Œéœ€è¦åŒå­¦ä»¬è‡ªå·±é€‰æ‹©è¯„ä¼°æ ‡å‡†
        # if avg_val_loss < best_val_loss:
        #     best_val_loss = avg_val_loss
        #     # ä¿å­˜æ¨¡å‹
        #     torch.save(
        #         {
        #             "epoch": epoch + 1,
        #             "img_encoder_state_dict": img_encoder.state_dict(),
        #             "txt_encoder_state_dict": txt_encoder.state_dict(),
        #             "optimizer_state_dict": optimizer.state_dict(),
        #             "tokenizer_vocab": tokenizer.word2idx,
        #             "best_val_loss": best_val_loss,
        #         },
        #         "chpts/best_clip_model.pth",
        #     )
        #     print(f"    > Best model updated at epoch {epoch + 1} ")

    plt.figure()
    plt.plot(range(1, epochs + 1), train_losses, label="Train Loss")
    plt.plot(range(1, epochs + 1), valid_losses, label="Valid Loss")
    plt.title("Training & Validation Loss over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("exp/images/loss_transformer.png")
    plt.show()

    # è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    # final_test_loss = evaluate(img_encoder, txt_encoder, test_dataloader, device)
    # print(f"Final Test Loss: {final_test_loss:.4f}")
    total_test_loss = 0.0
    pbar = tqdm(test_dataloader, desc="Final Test", unit="batch")
    with torch.no_grad():
        for images, captions_ids in pbar:
            images = images.to(device)
            captions_ids = captions_ids.to(device)

            image_embeds = img_encoder(images)
            text_embeds = txt_encoder(captions_ids)
            test_loss = contrastive_loss(image_embeds, text_embeds)
            total_test_loss += test_loss.item()
            pbar.set_postfix(loss=test_loss.item())

    avg_test_loss = total_test_loss / len(test_dataloader)
    print(f"Final Test Loss: {avg_test_loss:.4f}.")
    evaluate_top_k(img_encoder, txt_encoder, test_dataloader, device, topk=(1, 5, 10))


if __name__ == "__main__":
    main()
